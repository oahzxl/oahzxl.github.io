<!DOCTYPE html>
<html>
<meta property='og:title' content='Training Variable Sequences with Data-Centric Parallel '/>
<meta property='og:description' content='Training Variable Sequences with Data-Centric Parallel '/>
<meta property='og:url' content='https://oahzxl.github.io/DCP/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Training Variable Sequences with Data-Centric Parallel ">
  <meta name="keywords" content="Training Variable Sequences with Data-Centric Parallel ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Training Variable Sequences with Data-Centric Parallel </title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');

  .video-table td, .video-table th {
    padding-top: 2px;
    padding-bottom: 2px;
    padding-left: 4px;
    padding-right: 4px;
    font-weight: normal;
  }
  .first-col {
    width: 7%;
    vertical-align: middle;
  }
  .other-col {
    width: 31%;
  }
  body {
    font-family: "Lato", sans-serif;
    font-size: 1.1em;
  }
  .title.is-3 {
    font-weight: 900;
    font-size: 2.0rem;
  }
  .title.is-4 {
    font-weight: 700;
    font-size: 1.7rem;
  }
  .custom-emoji {
    width: 1em;
    height: 1em;
    display: inline-block;
    background-image: url('./static/images/pyramid.png');
    background-size: cover;
    vertical-align: middle;
    line-height: 1;
}

</style>


<body>
  <section class="hero is-small">
  <!-- <div class="hero-body"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <br><br>
        <h1 class="title is-2 publication-title" style="font-size: 2.4rem">
          Training Variable Sequences with <span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span> 
        </h1>
      
        <div class="is-size-5 publication-authors">
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=wrhNmbIAAAAJ&hl=en" target="_blank">Geng Zhang</a><b><sup>*</sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://oahzxl.github.io/" target="_blank">Xuanlei Zhao</a><b><sup>*</sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://kaiwang960112.github.io/" target="_blank">Kai Wang</a><b><sup>†</sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://www.comp.nus.edu.sg/~youy/" target="_blank">Yang You</a><b><sup>†</sup></b>
          </span>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block">VideoSys Team,&nbsp;&nbsp;National University of Singapore</span>
        </div>

        <!-- <div class="is-size-6 publication-authors">
          {zhangg, xuanlei, kai.wang, youy}@comp.nus.edu.sg
        </div> -->

        <div class="is-size-6 publication-authors">
          (* and † indicate equal contribution and correspondence)
        </div>

        <!-- <div class="is-size-5 publication-venue">
          in XXX
        </div> -->

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <!-- paper -->
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper (Coming Soon)</span>
              </a>
            </span>
            <!-- doc -->
            <span class="link-block">
              <a href="https://github.com/NUS-HPC-AI-Lab/OpenDiT/blob/master/docs/pab.md" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-book"></i>
                </span>
                <span>Doc</span>
                </a>
            </span>
            <!-- twitter -->
            <span class="link-block">
              <a href="https://x.com/oahzxl/status/1805939975420330298" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-twitter"></i>
                </span>
                <span>Twitter</span>
              </a>
            </span>
            <!-- bibtex -->
            <span class="link-block">
              <a href="#bibtex"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-obp"></i>
                </span>
                <span>BibTex</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
  <!-- </div> -->
</section>


<section class="hero is-small">
  <!-- <div class="hero-body"> -->
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <p>
        <br>
        <div class="columns is-centered has-text-centered">
          <b>Data should have its own parallel way, rather than being artificially defined. Let Data Choose its Parallel!</b>
        </div>
        <div style="text-align: right;">
          <span style="display: inline-block; width: 40px; border-top: 1px solid black; vertical-align: middle;"></span> VideoSys Team
        </div>
        <br>
        We introduce <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b> (<span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span>), a simple but effective approach to accelerate distributed training of varaible sequences (<i>e.g.,</i> videos).
        Unlike previous methods that fix training settings, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> <b>dyanmically adjusts parallelism and other configs driven by incoming data during runtime</b>.
        This method significantly reduces communication overhead and computational inefficiencies, achieving up to <b>2.1x</b> speedup.
        As a ease-of-use method, our method can enpower <b>any</b> models and most parallel methods within <b>minimal code changes (10 lines)</b>.
        <br>
      </p>
    </div>
  </div>
</div>

<div class="container is-max-desktop">
  <br>
  <video class="video" autoplay controls muted loop playsinline>
    <source src="./static/videos/dcp_video1_lowres.mp4" type="video/mp4">
  </video>
  <div class="columns is-centered has-text-centered">
    <span style="font-size: 0.8em; width: 100%; display: inline-block;"><br>Video 1: Overview of Data-Centric Parallel.</span>
  </div>
  <!-- </div> -->
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <br>
          <h2 class="title is-3">Method</h2>
          <h2 class="title is-4">Motivation <span style="font-size: 0.8em">- Reduce Imbalance and Communication Through Data-Centric</span></h2>
          <div class="content has-text-justified">
            <p>
              Variable sequences training is commonly used in various tasks, such as video generation (Sora <a href="#ref_sora">[1]</a>, Movie Gen <a href="#ref_moviegen">[2]</a>, Open-Sora <a href="#ref_opensora">[3]</a>, HunyuanVideo <a href="#ref_hunyuan">[4]</a>), text generation (Llama-3 <a href="#ref_llama3">[5]</a>), and scientific computing (AlphaFold <a href="#ref_alphafold">[6]</a>).
              Such strategy offers them two advantages: enhanced generation quality and flexible output sizes <a href="#ref_sora">[1]</a>.
            </p>

            <div class="content has-text-centered">
              <div id="compare">
                <img src="./static/images/dcp_compare.png" style="width: 100%;"><br>
              </div>
              <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 1: Comparison of parallel methods for variable sequences training including bucket parallel <a href="#ref_bucket">[7]</a>, packed parallel <a href="#ref_packed">[8]</a> and data-centric parallel (ours). \( Di \) refers to the \(i \)-th device.</span>
            </div>

            <p>
              We compare two common parallel methods for variable sequences training in Figure <a href="#compare">1</a>. Bucket parallel <a href="#ref_bucket">[7]</a> fixes sequence parallel size based on the longest sequence, adjusting batch sizes for balance.
              Packed parallel <a href="#ref_packed">[8]</a>, while also fixing sequence parallel size, concats multiple sequences in one batch.
            </p>

            <p>
              However, bucket parallel struggles with workload imbalance at small batch sizes and communication inefficiency for shorter sequences, while packed parallel, despite better load balancing, incurs unnecessary communication overhead and requires complex implementation changes. Both approaches fail to addresses two critical challenges in variable sequences training: necessary sequence parallel for long sequences and the unbalanced workloads from diverse sequence sizes, due to their fixed settings and lack of data awareness.
            </p>
          </div>
          <br>

          <h2 class="title is-4"><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span> <span style="font-size: 0.8em">- Let Data Choose its Parallel</span></h2>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <video class="video" autoplay controls muted loop playsinline>
                <source src="./static/videos/dcp_video_lowres.mp4" type="video/mp4">
              </video>
              <div class="columns is-centered has-text-centered">
                <span style="font-size: 0.8em; width: 100%; display: inline-block;"><br>Video 2: Detailed demonstration of Data-Centric Parallel.</span>
              </div>
              <div id="overview">
                <img src="./static/images/dcp_overview.png"><br>
              </div>
              <span style="font-size: 0.8em; width: 100%; display: inline-block;">Figure 2: Overview of Data-Centric Parallel (DCP). We take video data as an example. \( frame \) - number of frames, \( ar \) - aspect ratio, \( res \) - resolution, \( D_i\) -  the \( i \)-th device, \( bs \) - batch size, \( grad\ acc \) - gradient accumulation steps, and \( sp \) - sequence parallel size.</span>
            </div>
            <div class="content has-text-justified">
              <p>
                To address these challenges, we propose <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b>, an innovative approach that transforms parallel training with data awareness. Instead of fixing all configs during training, our method adaptively change parallel and other settings driven by the incoming data, significantly improving computational efficiency and reducing communication overhead.
              </p>
              <p>
                As shown in Figure <a href="#overview">2</a>, our method consists of two main stages: data profile and data-centric runtime. In data profile stage, we classify sequences by size and determine optimal settings for each sequence through fast dual-layer profile. At runtime, we first enqueue sequences into the batch until it's full, and then dynamically balance each sequences using two strategies:
                <li>
                  <b>DCP-intra</b>: adjusts the intrinsic settings (<i>e.g.,</i> batch size, sequence parallel size) of the sequence.
                </li>
                <li>
                  <b>DCP-inter</b>: optimizes among multiple sequences (<i>e.g.,</i> gradient accumulation) without changing intrinsic settings.
                </li>
              </p>
            </div>
          </div>
          <br>

          <h2 class="title is-4">Pyramid Activation Checkpointing <span style="font-size: 0.8em">- Adaptive Recompute for Variable Sequences</span></h2>
          <div class="content has-text-centered">
            <div id="ckpt" class="content has-text-centered">
              <img src="./static/images/dcp_ckpt.png" style="width: 70%;"><br>
              <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 3: Demonstration of pyramid activation checkpointing. We apply different ckpt ratios for different sizes of sequences, and trade sequence parallel for extra memory due to less ckpt.</span>
            </div>
            <div class="content has-text-justified">
              <p>
                As illustrated in Figure <a href="#ckpt">3</a>, sequence sizes are highly varied in variable sequences training, with short sequences using less memory and long sequences using more. Selective activation checkpointing are limited by longer sequences, as they require multi-gpu or even intra-node sequence parallelism if reducing recomputing.
              </p>
              <p>
                Based on <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span>, we propose Pyramid Activation Checkpointing (PAC), which adaptively applies different ckpt ratios based on sequence sizes. This approach significantly improves throughput for the shorter sequences by less recomputing, while maintaining communication overhead for longer sequences, which is even more beneficial in datasets where shorter sequences dominate.
              </p>
            </div>
          </div>
          <br>

          <h2 class="title is-4">Performance Modeling <span style="font-size: 0.8em">- How to Achieve Best Efficiency?</span></h2>
          <div class="content has-text-justified">

            <p>
              For various sequences training, optimization is a challenging task because it involves balancing multiple factors instead of communication alone. We introduce the ICC (Imbalance-Communication-Computation) metric for variable sequences training, which identifies three fundamental factors that determine the overall system performance: workload imbalance, communication overhead, and computation degradation. It can be formally expressed as:
            </p>
            <p>
              $$
              ICC(\lambda) = {\eta_{imb}(\lambda)}^{\alpha} \cdot {\eta_{comm}(\lambda)}^{\beta} \cdot {\eta_{comp}(\lambda)}^{\gamma}
              $$
              $$
              \eta_{imb}(\lambda) = \frac{T_{total}(\lambda)}{T_{idle}(\lambda)},\ \ \eta_{comm}(\lambda) = \frac{T_{comp}(\lambda)}{T_{comm}(\lambda)},\ \ \eta_{comp}(\lambda) = \frac{T_{optimal}(\lambda)}{T_{degrad}(\lambda)}
              $$
            </p>
            <p>
              where \( \lambda \) is the sequence distribution, \( T_{total} \) is the total time, \( T_{idle} \) is the idle time, \( T_{comp} \) is the computation time, \( T_{comm} \) is the optiaml communication time, \( T_{optimal} \) is the optimal computation time, and \( T_{degrad} \) is the computation degradation time.
            </p>

            <div class="content has-text-centered"> 
              <div id="roofline">
                <img src="./static/images/roofline.png" style="width: 50%;"><br>
              </div>
              <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 4: Roofline model of variable sequences training.</span>
            </div>
            <p>
              Based on ICC metric, we propose a roofline performance model to characterize and analyze the training performance of variable-length sequences, as illustrated in Figure <a href="#roofline">4</a>. By optimizing ICC, we can achieve the best performance for each method.
              <li>
                The semi-optimal curve demonstrates the maximum achievable performance through intra-sequence balancing. To achieve better balance, it inevitable introduces reduced computational efficiency due to smaller batch sizes, and increased communication overhead for additional iterations.
              </li>
              <li>
                The optimal curve represents the peak performance through inter-sequence balancing, showcasing the theoretical maximum efficiency of our proposed method.
              </li>
            </p>

          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Evaluations</h2>
          <p>
            We conduct experiments on NVIDIA H100 GPUs connected by NVLink and InfiniBand. The evaluation is performed using on Open-Sora v1.2 (1.1B). The sequence distribution follows common video size and variations. We use bucket parallel used in Open-Sora as the baseline method.
          </p>
          <br>
        
          <h2 class="title is-4">End-to-End Speedups <span style="font-size: 0.8em">- How Much Can DCP Speed Up?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/speedup.png">
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 5: End-to-end speedups of DCP compared with other parallel methods.</span>
          </div>
          <p>
            Measured throughput of <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span>  across different sequence length distributions using 32 NVIDIA H100 GPUs. The results demonstrated significant performance improvements: when short sequences are predominant, DCP achieved speedups of up to 2.48x. Notably, even in scenarios dominated by long sequences, DCP maintained substantial performance gains with speedups of 1.28x.
          </p>
          <br>

          <h2 class="title is-4">Imbalance Analysis <span style="font-size: 0.8em">- How is Imbalance with DCP?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/bubble.png" style="width: 50%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 6: Imbalance analysis of DCP. It is able to maintain low imbalance ratio as the number of GPUs increases.</span>
          </div>
          <p>
            Figure <a href="#bubble">6</a> illustrates how the imbalance ratio changes with increasing number of GPUs. The baseline method shows faster growth in imbalance, reaching 16.4% when scaled to 32 GPUs. In contrast, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> demonstrates superior performance, maintaining the lowest imbalance ratio and exhibiting slower growth as GPU count increases. 
          </p>
          <br>

          <h2 class="title is-4">Parallel Analysis <span style="font-size: 0.8em">- Why Should We Determine Parallel Size by Data?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/exp_sp.png" style="width: 100%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 7: Parallel analysis of DCP. It is able to achieve better performance than fixed parallel size.</span>
          </div>
          <p>
            Figure <a href="#exp_sp">7</a> illustrates the optimal parallel size and performance improvements across different sequence types. As expected, the optimal parallel size strongly correlates with the total sequence length. Notably, while one might assume smaller parallel sizes would be optimal, larger parallel sizes sometimes can  enhance computational efficiency and achieve better workload balance.
          </p>
          <br>

          <h2 class="title is-4">Recompute Analysis <span style="font-size: 0.8em">- When Can Pyramid Activation Checkpointing Help?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/exp_ckpt.png" style="width: 50%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 8: Recompute analysis of DCP. PAC achieves significant speedup across most sequence lengths.</span>
          </div>
          <p>
            Figure <a href="#exp_ckpt">8</a> shows the speedup of Pyramid Activation Checkpointing. Our results show that it achieves significant speedup across most sequence lengths, with particularly notable performance gains for shorter sequences, which dominate most datasets.
          </p>
          <br>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related works</h2>
          <div class="content has-text-justified">
            <ol>
              <li>
                <!-- 1 -->
                <div id="ref_sora">
                  Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. "<a href="https://openai.com/research/video-generation-models-as-world-simulators" target="_blank">Video generation models as world simulators</a>." 2024.
                </div>
              </li>
              <li>
                <!-- 2 -->
                <div id="ref_moviegen">
                  Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas et al. "<a href="https://arxiv.org/abs/2410.13720" target="_blank">Movie Gen: A Cast of Media Foundation Models</a>." arXiv, 2024.
                </div>
              </li>
              <li>
                <!-- 3 -->
                <div id="ref_opensora">
                  Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. "<a href="https://github.com/hpcaitech/Open-Sora" target="_blank">Open-Sora: Democratizing Efficient Video Production for All</a>." GitHub, 2024.
                </div>
              </li>
              <li>
                <!-- 4 -->
                <div id="ref_hunyuan">
                  Hunyuan Foundation Model Team. "<a href="https://github.com/Tencent/HunyuanVideo">HunyuanVideo: A Systematic Framework For Large Video Generation Model</a>". arXiv, 2024.
                </div>
              </li>
              <li>
                <!-- 5 -->
                <div id="ref_llama3">
                  Llama Team. "<a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a>". arXiv, 2024.
                </div>
              </li>
              <li>
                <!-- 6 -->
                <div id="ref_alphafold">
                  Jumper, J., Evans, R., Pritzel, A. et al. "<a href="https://www.nature.com/articles/s41586-021-03819-2">Highly accurate protein structure prediction with AlphaFold</a>". Nature, 596.
                </div>
              </li>
              <li>
                <!-- 7 -->
                <div id="ref_bucket">
                  Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi et al. "<a href="https://arxiv.org/abs/2403.03206" target="_blank">Scaling rectified flow transformers for high-resolution image synthesis</a>." ICML, 2024.
                </div>
              </li>
              <li>
                <!-- 8 -->
                <div id="ref_packed">
                  Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner et al. "<a href="https://arxiv.org/abs/2307.06304" target="_blank">Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution</a>." NeurIPS, 2024.
                </div>
              </li>
              <li>
                Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, and Yang You. "<a href="https://arxiv.org/abs/2403.10266" target="_blank">Dsp: Dynamic sequence parallelism for multi-dimensional transformers</a>." arXiv, 2024.
              </li>
              <li>
                Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. "<a href="https://arxiv.org/abs/2309.14509" target="_blank">Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models</a>." arXiv, 2023.
              </li>
              <li>
                Tailing Yuan, Yuliang Liu, Xucheng Ye, Shenglong Zhang, Jianchao Tan, Bin Chen, Chengru Song, and Di Zhang. "<a href="https://www.usenix.org/system/files/atc24-yuan.pdf" target="_blank">Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</a>." ATC, 2024.
              </li>
              <li>
                Jiannan Wang, Jiarui Fang, Aoyu Li, and PengCheng Yang. "<a href="https://arxiv.org/abs/2405.14430" target="_blank">PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models</a>." arXiv, 2024.
              </li>
              <li>
                Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. "<a href="https://arxiv.org/abs/1909.08053" target="_blank">Megatron-lm: Training multi-billion parameter language models using model parallelism</a>." arXiv, 2019.
              </li>
              <li>
                Jiarui Fang, and Shangchun Zhao. "<a href="https://arxiv.org/abs/2405.07719" target="_blank">A Unified Sequence Parallelism Approach for Long Context Generative AI</a>." arXiv, 2024.
              </li>
              <li>
                Shenggui Li, Fuzhao Xue, Yongbin Li and Yang You. "<a href="https://arxiv.org/abs/2105.13120" target="_blank">Sequence Parallelism: Long Sequence Training from System Perspective</a>." ACL, 2021.
              </li>
              <li>
                Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li, Faming Wu, and Bin Cui. "<a href="https://arxiv.org/abs/2412.01523" target="_blank">Data-Centric and Heterogeneity-Adaptive Sequence Parallelism for Efficient LLM Training</a>." arXiv, 2024.
              </li>
              <li>
                Hao Ge, Fangcheng Fu, Haoyang Li, Xuanyu Wang, Sheng Lin, Yujie Wang, Xiaonan Nie, Hailin Zhang, Xupeng Miao, and Bin Cui. "<a href="https://dl.acm.org/doi/abs/10.1145/3694715.3695969" target="_blank">Enabling Parallelism Hot Switching for Efficient Training of Large Language Models</a>." SOSP, 2024.
              </li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{zhang2024dcp,
  title={Training Variable Sequences with Data-Centric Parallel},
  author={Geng Zhang and Xuanlei Zhao and Kai Wang and Yang You},
  year={2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
