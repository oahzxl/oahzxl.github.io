<!DOCTYPE html>
<html>
<meta property='og:title' content='Training Any-Size Videos with Data-Centric Parallel '/>
<meta property='og:description' content='Training Any-Size Videos with Data-Centric Parallel '/>
<meta property='og:url' content='https://oahzxl.github.io/DCP/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Training Any-Size Videos with Data-Centric Parallel ">
  <meta name="keywords" content="Training Any-Size Videos with Data-Centric Parallel ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Training Any-Size Videos with Data-Centric Parallel </title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');

  .video-table td, .video-table th {
    padding-top: 2px;
    padding-bottom: 2px;
    padding-left: 4px;
    padding-right: 4px;
    font-weight: normal;
  }
  .first-col {
    width: 7%;
    vertical-align: middle;
  }
  .other-col {
    width: 31%;
  }
  body {
    font-family: "Lato", sans-serif;
    font-size: 1.1em;
  }
  .title.is-3 {
    font-weight: 900;
    font-size: 2.0rem;
  }
  .title.is-4 {
    font-weight: 700;
    font-size: 1.7rem;
  }
  .custom-emoji {
    width: 1em;
    height: 1em;
    display: inline-block;
    background-image: url('./static/images/pyramid.png');
    background-size: cover;
    vertical-align: middle;
    line-height: 1;
}

</style>


<body>
  <section class="hero is-small">
  <!-- <div class="hero-body"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <br><br>
        <h1 class="title is-2 publication-title" style="font-size: 2.4rem">
          Training Any-Size Videos with <span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span> 
        </h1>
      
        <div class="is-size-5 publication-authors">
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=wrhNmbIAAAAJ&hl=en" target="_blank">Geng Zhang</a><b><sup>*</sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://oahzxl.github.io/" target="_blank">Xuanlei Zhao</a><b><sup>*</sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://kaiwang960112.github.io/" target="_blank">Kai Wang</a><b><sup>â€ </sup></b>,&nbsp;</span>
          <span class="author-block">
            <a href="https://www.comp.nus.edu.sg/~youy/" target="_blank">Yang You</a><b><sup>â€ </sup></b>
          </span>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block">National University of Singapore</span>
        </div>

        <!-- <div class="is-size-6 publication-authors">
          {zhangg, xuanlei, kai.wang, youy}@comp.nus.edu.sg
        </div> -->

        <div class="is-size-6 publication-authors">
          (* and â€  indicate equal contribution and correspondence)
        </div>

        <!-- <div class="is-size-5 publication-venue">
          in XXX
        </div> -->

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <!-- paper -->
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper (Coming Soon)</span>
              </a>
            </span>
            <!-- doc -->
            <span class="link-block">
              <a href="https://github.com/NUS-HPC-AI-Lab/OpenDiT/blob/master/docs/pab.md" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-book"></i>
                </span>
                <span>Doc</span>
                </a>
            </span>
            <!-- twitter -->
            <span class="link-block">
              <a href="https://x.com/oahzxl/status/1805939975420330298" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-twitter"></i>
                </span>
                <span>Twitter</span>
              </a>
            </span>
            <!-- bibtex -->
            <span class="link-block">
              <a href="#bibtex"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-obp"></i>
                </span>
                <span>BibTex</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
  <!-- </div> -->
</section>


<section class="hero is-small">
  <!-- <div class="hero-body"> -->
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <p>
        <b>New Paradigm for Any-Size Video Training <span style="font-size: 1.0em">ðŸš€</span>: Let Data Choose its Parallel!</b>
        We introduce <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b> (<span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span>), a simple but effective approach to accelerate distributed training of any-size videos.
        Unlike previous methods that fix training settings, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> <b>dyanmically adjusts parallelism and other configs driven by incoming data during runtime</b>.
        This method significantly reduces communication overhead and computational inefficiencies, achieving up to <b>2.1x</b> speedup.
        As a ease-of-use method, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> can enpower <b>any</b> video models and parallel methods with <b>minimal code changes</b>.
        <br>
      </p>
    </div>
  </div>
</div>

<div class="container is-max-desktop">
  <br>
  <video class="video" autoplay controls muted loop playsinline>
    <source src="./static/videos/dcp_video_lowres.mp4" type="video/mp4">
  </video>
  <div class="columns is-centered has-text-centered">
    <span style="font-size: 0.8em; width: 100%; display: inline-block;"><br>Video 1: Video demonstration of Data-Centric Parallel.</span>
  </div>
  <!-- </div> -->
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <br>
          <h2 class="title is-3">Method</h2>
          <h2 class="title is-4">Motivation <span style="font-size: 0.8em">- Reduce Imbalance and Communication Through Data-Centric</span></h2>
          <div class="content has-text-justified">
            <p>
              Sora <a href="#ref_sora">[1]</a> and its successors (Movie Gen <a href="#ref_moviegen">[2]</a>, Open-Sora <a href="#ref_opensora">[3]</a>, Vchitect <a href="#ref_vchitect">[4]</a>) have recently achieved remarkable progress in video generation. They train on variable size videos (<i>i.e.,</i> num frames, aspect ratio, and resolution), which offers two advantages: enhanced generation quality and flexible output video sizes <a href="#ref_sora">[1]</a>.
            </p>

            <div class="content has-text-centered">
              <div id="compare">
                <img src="./static/images/dcp_compare.png" style="width: 100%;"><br>
              </div>
              <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 1: Comparison of parallel methods for any-size video training including bucket parallel <a href="#ref_bucket">[5]</a>, packed parallel <a href="#ref_packed">[6]</a> and data-centric parallel (ours). \( Di \) refers to the \(i \)-th device.</span>
            </div>

            <p>
              We compare two common parallel methods for any-size videos training in Figure <a href="#compare">1</a>. Bucket parallel <a href="#ref_bucket">[5]</a> fixes sequence parallel size based on the longest video, adjusting batch sizes for balance.
              Packed parallel <a href="#ref_packed">[6]</a>, while also fixing sequence parallel size, concats multiple videos in one batch.
            </p>

            <p>
              However, bucket parallel struggles with workload imbalance at small batch sizes and communication inefficiency for shorter videos, while packed parallel, despite better load balancing, incurs unnecessary communication overhead and requires complex implementation changes. Both approaches fail to addresses two critical challenges in any-size video training: necessary sequence parallel for long videos and the unbalanced workloads from diverse video sizes, due to their fixed settings and lack of data awareness.
            </p>
          </div>
          <br>

          <h2 class="title is-4"><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span> <span style="font-size: 0.8em">- Let Data Choose its Parallel</span></h2>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <div id="overview">
                <img src="./static/images/dcp_overview.png"><br>
              </div>
              <span style="font-size: 0.8em; width: 100%; display: inline-block;">Figure 2: Overview of Data-Centric Parallel (DCP). \( frame \) - number of frames, \( ar \) - aspect ratio, \( res \) - resolution, \( D_i\) -  the \( i \)-th device, \( bs \) - batch size, \( grad\ acc \) - gradient accumulation steps, and \( sp \) - sequence parallel size.</span>
            </div>
            <div class="content has-text-justified">
              <p>
                To address these challenges, we propose <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b>, an innovative approach that transforms parallel training with data awareness. Instead of fixing all configs during training, our method adaptively change parallel and other settings driven by the incoming data, significantly improving computational efficiency and reducing communication overhead.
              </p>
              <p>
                As shown in Figure <a href="#overview">2</a>, our method consists of two main stages: data profile and data-centric runtime. In data profile stage, we classify videos by size and determine optimal settings for each sequence through fast dual-layer profile. At runtime, we first enqueue videos into the batch until it's full, and then dynamically balance each sequences using two strategies:
                <li>
                  <b>Intra-data balance</b>: adjusts the intrinsic settings (<i>e.g.,</i> batch size, sequence parallel size) of the sequence.
                </li>
                <li>
                  <b>Inter-data balance</b>: optimizes among multiple sequences (<i>e.g.,</i> gradient accumulation) without changing intrinsic settings.
                </li>
              </p>
            </div>
          </div>
          <br>

          <h2 class="title is-4">Pyramid Activation Checkpointing <span style="font-size: 0.8em">- Adaptive Recompute for Any-Size Videos</span></h2>
          <div class="content has-text-centered">
            <div id="ckpt" class="content has-text-centered">
              <img src="./static/images/dcp_ckpt.png" style="width: 70%;"><br>
              <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 3: Demonstration of pyramid activation checkpointing. We apply different ckpt ratios for different sizes of videos, and trade sequence parallel for extra memory due to less ckpt.</span>
            </div>
            <div class="content has-text-justified">
              <p>
                As illustrated in Figure <a href="#ckpt">3</a>, in any-size videos training, video sizes and their memory consumption are highly varied, with short videos using less memory and long videos using more. Selective activation checkpointing are limited by longer videos, as longer videos typically require multi-gpu or even intra-node sequence parallelism if reducing recomputing.
              </p>
              <p>
                Building upon our data-centric parallel approach, we propose pyramid activation checkpointing, which adaptively applies different ckpt ratios based on video sizes. This approach significantly improves throughput for the shorter videos by less recomputing, while maintaining communication overhead for longer videos, which is even more beneficial in datasets where shorter videos dominate.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Evaluations</h2>
          
          <h2 class="title is-4">End-to-End Speedups <span style="font-size: 0.8em">- How Fast Can DCP Train?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/speedup.png">
          </div>
          <div class="content has-text-justified">
            <p>
              Measured total latency of <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> for different models for generating a single video on 8 NVIDIA H100 GPUs. When utilizing a single GPU, we achieve a speedup ranging from 1.26x to 1.32x, which remains stable across different schedulers. Scaling to multiple GPUs, our method achieves a speedup of up to 10.6x, which almost linearly scales with the number of GPUs due to our efficient improvement of sequence parallel.
            </p>
          </div>

          <h2 class="title is-4">Parallel Analysis <span style="font-size: 0.8em">- Why Should We Determine Parallel Size by Data?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/final_heatmap.png" style="width: 90%;"><br>
          </div>

          <h2 class="title is-4">Recompute Analysis <span style="font-size: 0.8em">- When Can Pyramid Activation Checkpointing Help?</span></h2>
          <div class="content has-text-centered">
            <img src="./static/images/final_3d_speedup.png" style="width: 90%;"><br>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related works</h2>
          <div class="content has-text-justified">
            <ol>
              <li>
                <!-- 1 -->
                <div id="ref_sora">
                  Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. "<a href="https://openai.com/research/video-generation-models-as-world-simulators" target="_blank">Video generation models as world simulators</a>." 2024.
                </div>
              </li>
              <li>
                <!-- 2 -->
                <div id="ref_moviegen">
                  Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas et al. "<a href="https://arxiv.org/abs/2410.13720" target="_blank">Movie Gen: A Cast of Media Foundation Models</a>." arXiv, 2024.
                </div>
              </li>
              <li>
                <!-- 3 -->
                <div id="ref_opensora">
                  Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. "<a href="https://github.com/hpcaitech/Open-Sora" target="_blank">Open-Sora: Democratizing Efficient Video Production for All</a>." GitHub, 2024.
                </div>
              </li>
              <li>
                <!-- 4 -->
                <div id="ref_vchitect">
                  Vchitect Team. "<a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models</a>". GitHub, 2024.
                </div>
              </li>
              <li>
                <!-- 5 -->
                <div id="ref_bucket">
                  Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi et al. "<a href="https://arxiv.org/abs/2403.03206" target="_blank">Scaling rectified flow transformers for high-resolution image synthesis</a>." ICML, 2024.
                </div>
              </li>
              <li>
                <!-- 6 -->
                <div id="ref_packed">
                  Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner et al. "<a href="https://arxiv.org/abs/2307.06304" target="_blank">Patch nâ€™pack: Navit, a vision transformer for any aspect ratio and resolution</a>." NeurIPS, 2024.
                </div>
              </li>
              <li>
                Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, and Yang You. "<a href="https://arxiv.org/abs/2403.10266" target="_blank">Dsp: Dynamic sequence parallelism for multi-dimensional transformers</a>." arXiv, 2024.
              </li>
              <li>
                Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. "<a href="https://arxiv.org/abs/2309.14509" target="_blank">Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models</a>." arXiv, 2023.
              </li>
              <li>
                Tailing Yuan, Yuliang Liu, Xucheng Ye, Shenglong Zhang, Jianchao Tan, Bin Chen, Chengru Song, and Di Zhang. "<a href="https://www.usenix.org/system/files/atc24-yuan.pdf" target="_blank">Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</a>." ATC, 2024.
              </li>
              <li>
                Jiannan Wang, Jiarui Fang, Aoyu Li, and PengCheng Yang. "<a href="https://arxiv.org/abs/2405.14430" target="_blank">PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models</a>." arXiv, 2024.
              </li>
              <li>
                Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. "<a href="https://arxiv.org/abs/1909.08053" target="_blank">Megatron-lm: Training multi-billion parameter language models using model parallelism</a>." arXiv, 2019.
              </li>
              <li>
                Jiarui Fang, and Shangchun Zhao. "<a href="https://arxiv.org/abs/2405.07719" target="_blank">A Unified Sequence Parallelism Approach for Long Context Generative AI</a>." arXiv, 2024.
              </li>
              <li>
                Shenggui Li, Fuzhao Xue, Yongbin Li and Yang You. "<a href="https://arxiv.org/abs/2105.13120" target="_blank">Sequence Parallelism: Long Sequence Training from System Perspective</a>." ACL, 2021.
              </li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{zhang2024dcp,
  title={Training Any-Size Videos with Data-Centric Parallel},
  author={Geng Zhang and Xuanlei Zhao and Kai Wang and Yang You},
  year={2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
