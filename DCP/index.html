<!DOCTYPE html>
<html>
<meta property='og:title' content='Training Any-Size Videos with Data-Centric Parallel '/>
<meta property='og:description' content='Training Any-Size Videos with Data-Centric Parallel '/>
<meta property='og:url' content='https://oahzxl.github.io/DCP/'/>
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website'/>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Training Any-Size Videos with Data-Centric Parallel ">
  <meta name="keywords" content="Training Any-Size Videos with Data-Centric Parallel ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Training Any-Size Videos with Data-Centric Parallel </title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">
  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Patrick+Hand|Google+Sans|Noto+Sans|Castoro|Lato|Open+Sans&effect=shadow-multiple|emboss|3d"> 
  <link rel="icon" href="./static/images/pyramid.png" type="image/x-icon">
  <link rel="shortcut icon" href="./static/images/pyramid.png" type="image/x-icon">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');

  .video-table td, .video-table th {
    padding-top: 2px;
    padding-bottom: 2px;
    padding-left: 4px;
    padding-right: 4px;
    font-weight: normal;
  }
  .first-col {
    width: 7%;
    vertical-align: middle;
  }
  .other-col {
    width: 31%;
  }
  body {
    font-family: "Lato", sans-serif;
    font-size: 1.1em;
  }
  .title.is-3 {
    font-weight: 900;
    font-size: 2.0rem;
  }
  .title.is-4 {
    font-weight: 700;
    font-size: 1.7rem;
  }
  .custom-emoji {
    width: 1em;
    height: 1em;
    display: inline-block;
    background-image: url('./static/images/pyramid.png');
    background-size: cover;
    vertical-align: middle;
    line-height: 1;
}

</style>


<body>
  <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <br><br>
          <h1 class="title is-2 publication-title" style="font-size: 2.4rem">
            Training Any-Size Videos with <span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span> 
          </h1>
        
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=wrhNmbIAAAAJ&hl=en" target="_blank">Geng Zhang</a><b><sup>*</sup></b>,&nbsp;</span>
            <span class="author-block">
              <a href="https://oahzxl.github.io/" target="_blank">Xuanlei Zhao</a><b><sup>*</sup></b>,&nbsp;</span>
            <span class="author-block">
              <a href="https://kaiwang960112.github.io/" target="_blank">Kai Wang</a><b><sup>â€ </sup></b>,&nbsp;</span>
            <span class="author-block">
              <a href="https://www.comp.nus.edu.sg/~youy/" target="_blank">Yang You</a><b><sup>â€ </sup></b>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">National University of Singapore</span>
          </div>

          <!-- <div class="is-size-6 publication-authors">
            {zhangg, xuanlei, kai.wang, youy}@comp.nus.edu.sg
          </div> -->

          <div class="is-size-6 publication-authors">
            (* and â€  indicate equal contribution and correspondence)
          </div>

          <!-- <div class="is-size-5 publication-venue">
            in XXX
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- paper -->
              <span class="link-block">
                <a class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <!-- doc -->
              <span class="link-block">
                <a href="https://github.com/NUS-HPC-AI-Lab/OpenDiT/blob/master/docs/pab.md" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-book"></i>
                  </span>
                  <span>Doc</span>
                  </a>
              </span>
              <!-- twitter -->
              <span class="link-block">
                <a href="https://x.com/oahzxl/status/1805939975420330298" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
              <!-- bibtex -->
              <!-- <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <p>
          <b>Revolutionizing Any-Size Video Training: Faster Than Ever Before! <span style="font-size: 1.3em">ðŸš€</span>!</b>
          We introduce <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b> (<span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span>), a simple but effective approach to accelerate distributed training of any-size video generation.
          Unlike previous methods, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> <b>adapt parallelism and other settings based on incoming videos</b>. 
          This method significantly reduces communication overhead and computational inefficiencies, achieving up to <b>2.1x</b> speedup.
          As a ease-of-use method, <span style="color: #e2001d;">D</span><span style="color: #a80057;">C</span><span style="color: #6d0092;">P</span> can enpower <b>any</b> video models and parallel methods with <b>minimal code changes</b>.
          <br>
        </p>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <video class="video" autoplay controls muted loop playsinline>
      <source src="./static/videos/compare.mp4" type="video/mp4">
    </video>
    <div class="columns is-centered">
      <div class="content has-text-centered">
        <br>
        <span style="font-size: 0.8em; width: 80%; display: inline-block;">Video 1: This should be a video demo.</span>
        <br>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            <a href="https://openai.com/index/video-generation-models-as-world-simulators/" target="_blank">Sora</a> and its successors (<a href="https://ai.meta.com/research/publications/movie-gen-a-cast-of-media-foundation-models/" target="_blank">Movie Gen</a>, <a href="https://github.com/hpcaitech/Open-Sora" target="_blank">Open-Sora</a>, <a href="https://vchitect.intern-ai.org.cn/" target="_blank">Vchitect</a>) have recently achieved remarkable progress in video generation. A notable feature of these models is their training on variable size video datasets, which offers two advantages: enhanced generation quality and flexible output video sizes. However, it introduces significant computational challenges due to the necessary sequence parallelization for long videos and the unbalanced workloads from diverse video sizes.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/dcp_compare.png" style="width: 100%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 1: Comparison of parallel methods for any-size video training including <a href="https://github.com/hpcaitech/Open-Sora" target="_blank">bucket parallel</a>, <a href="https://arxiv.org/abs/2307.06304" target="_blank">packed parallel</a> and data-centric parallel (ours). \( Di \) refers to the \(i \)-th device.</span>
          </div>
          <p>
            As shown in Figure 1, we compare two common parallel methods and ours for any-size video training. <a href="https://github.com/hpcaitech/Open-Sora" target="_blank">Bucket parallel</a> fixes sequence parallel size based on the longest video, adjusting batch sizes for balance. While straightforward, it suffers from inefficient communication for shorter videos and struggles to achieve balance for small batches.
            <a href="https://arxiv.org/abs/2307.06304" target="_blank">Packed parallel</a>, while also fixing sequence parallel size, concats multiple videos in one batch. This method achieves better balance but introduces redundant communication and requires complex code modifications.
          </p>
          <p>
            To solve these challenges, we propose <b><span style="color: #ff0000;">D</span><span style="color: #f80007;">a</span><span style="color: #f0000f;">t</span><span style="color: #e90016;">a</span><span style="color: #e2001d;">-</span><span style="color: #db0025;">C</span><span style="color: #d3002c;">e</span><span style="color: #cc0033;">n</span><span style="color: #c5003a;">t</span><span style="color: #bd0042;">r</span><span style="color: #b60049;">i</span><span style="color: #af0050;">c</span><span style="color: #a70058;"> </span><span style="color: #a0005f;">P</span><span style="color: #990066;">a</span><span style="color: #92006e;">r</span><span style="color: #8a0075;">a</span><span style="color: #83007c;">l</span><span style="color: #7c0083;">l</span><span style="color: #74008b;">e</span><span style="color: #6d0092;">l</span></b>. Instead of fixing parallel configs, our method adaptively change parallelism and other settings driven by the incoming data, significantly improving computational efficiency and reducing communication overhead.
          </p>
        </div>

        <h2 class="title is-3">Data-Centric Parallel</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="./static/images/method.png"><br>
            <span style="font-size: 0.8em; width: 100%; display: inline-block;">Figure 2: We propose pyramid attention broadcast (shown on the right side) which sets different broadcast ranges for three attentions based on their differences. The smaller the variation in attention, the broader the broadcast range. During runtime, we broadcast attention results to the next several steps (shown on the left side) to avoid redundant attention computations. \( x_t \) refers to the features at timestep \( t \).</span>
          </div>
          <div class="content has-text-justified">
            Building on these insights, we propose <span class="custom-emoji"></span><b><span style="color: #ff0000;">P</span><span style="color: #f80007;">y</span><span style="color: #f1000e;">r</span><span style="color: #ea0015;">a</span><span style="color: #e4001c;">m</span><span style="color: #dd0022;">i</span><span style="color: #d60029;">d</span> <span style="color: #cf0030;">A</span><span style="color: #c80037;">t</span><span style="color: #c1003e;">t</span><span style="color: #ba0045;">e</span><span style="color: #b3004c;">n</span><span style="color: #ad0053;">t</span><span style="color: #a60059;">i</span><span style="color: #9f0060;">o</span><span style="color: #980067;">n</span> <span style="color: #91006e;">B</span><span style="color: #8a0075;">r</span><span style="color: #83007c;">o</span><span style="color: #7c0083;">a</span><span style="color: #76008a;">d</span><span style="color: #6f0090;">c</span><span style="color: #680097;">a</span><span style="color: #61009e;">s</span><span style="color: #5a00a5;">t</span></b> to alleviate unnecessary attention computations. In the middle segment, where attentions show minor differences, we can broadcast one diffusion step's attention outputs to several subsequent steps, thereby significantly reducing computational costs. Furthermore, for more efficient computation and minimum quality loss, we set varied broadcast ranges for different attentions based on their stability and differences. This simple yet effective strategy achieves up to a 35% speedup with negligible quality loss, even without post-training.
          </div>
        </div>

        <h2 class="title is-3">Heterogeneous Activation Checkpointing</h2>
        <div class="content has-text-centered">
          <div class="content has-text-centered">
            <img src="./static/images/parallel.png" style="width: 70%;"><br>
            <span style="font-size: 0.8em; width: 80%; display: inline-block;">Figure 3: Comparison between original Dynamic Sequence Parallel (DSP) and ours. When temporal attention is broadcasted, we can avoid all communication.</span>
          </div>
          <div class="content has-text-justified">
            To further enhance video generation speed, we improve sequence parallel based on <a href="https://arxiv.org/abs/2403.10266" target="_blank">Dynamic Sequence Parallelism</a> (DSP). Sequence parallelism segments videos into different parts across multiple GPUs, reducing the workload on each GPU and decreasing generation latency. However, DSP introduces significant communication overhead, requiring two all-to-all communications for temporal attention. By broadcasting temporal attention in <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span>, we eliminate these communications, as temporal attention no longer needs to be computed. This results in a significant reduction in communication overhead by over 50%, enabling more efficient distributed inference for real-time video generation.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluations</h2>
        
        <h2 class="title is-4">Speedups</h2>
        <div class="content has-text-centered">
          <img src="./static/images/speedup.png">
        </div>
        <div class="content has-text-justified">
          <p>
            Measured total latency of <span style="color: #e2001d;">P</span><span style="color: #a80057;">A</span><span style="color: #6d0092;">B</span> for different models for generating a single video on 8 NVIDIA H100 GPUs. When utilizing a single GPU, we achieve a speedup ranging from 1.26x to 1.32x, which remains stable across different schedulers. Scaling to multiple GPUs, our method achieves a speedup of up to 10.6x, which almost linearly scales with the number of GPUs due to our efficient improvement of sequence parallel.
          </p>
        </div>
      </div>
    </div>

    <h2 class="title is-4">Quanlitive Results</h2>
    <div class="container is-max-desktop">
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video1.mp4" type="video/mp4">
      </video>
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video2.mp4" type="video/mp4">
      </video>
      <video class="video" autoplay controls muted loop playsinline>
        <source src="./static/videos/video3.mp4" type="video/mp4">
      </video>
      <br>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">        
        <div class="content has-text-justified">
          <h2 class="title is-4">Quantitive Results</h2>
          <div class="content has-text-centered">
            <img src="./static/images/eval.png" style="width: 40%;"><br>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related works</h2>
        <div class="content has-text-justified">
          <p>
            <li>
              Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. <a href="https://github.com/hpcaitech/Open-Sora">Open-Sora: Democratizing Efficient Video Production for All</a>. GitHub, 2024.
            </li>
            <li>
              PKU-Yuan Lab and Tuzhan AI etc. <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">Open-Sora-Plan</a>. GitHub, 2024.
            </li>
            <li>
              Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. <a href="https://arxiv.org/abs/2401.03048">Latte: Latent Diffusion Transformer for Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Xuanlei Zhao, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, and Yang You. <a href="https://arxiv.org/abs/2401.03048">DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</a>. arXiv, 2024.
            </li>
            <li>
              Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. <a href="https://arxiv.org/abs/2402.19481">DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</a>. In CVPR, 2024.
            </li>
            <li>
              Xinyin Ma, Gongfan Fang, and Xinchao Wang. <a href="https://arxiv.org/abs/2312.00858">DeepCache: Accelerating Diffusion Models for Free</a>. In CVPR, 2024.
            </li>
            <li>
              Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. <a href="https://arxiv.org/abs/2311.17982">VBench: Comprehensive Benchmark Suite for Video Generative Models</a>. In CVPR, 2024.
            </li>
            <li>
              David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. <a href="https://arxiv.org/abs/2406.11816">Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. <a href="https://arxiv.org/abs/2209.14792">Make-A-Video: Text-to-Video Generation without Text-Video Data</a>. In ICLR, 2023.
            </li>
            <li>
              Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. <a href="https://arxiv.org/abs/2401.12945">Lumiere: A Space-Time Diffusion Model for Video Generation</a>. arXiv, 2024.
            </li>
            <li>
              Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. <a href="https://arxiv.org/abs/2304.08818">Align your Latents:
                High-Resolution Video Synthesis with Latent Diffusion Models</a>. In CVPR, 2023.
            </li>
            <li>
              Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun. <a href="https://arxiv.org/abs/2403.13248">Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</a>. In CVPR, 2023.
            </li>
          </p>
        </div>

        <!-- <h2 class="title is-3">Acknowledgments</h2>
        <div class="content has-text-justified">
          <p>
            Xuanlei, Xiaolong, and Kai contribute equally to this work. Kai and Yang are equal advising.
          </p>
        </div> -->
      </div>
    </div>
  </div>

</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@misc{zhao2024pab,
      title={Training Any-Size Videos with Data-Centric Parallel },
      author={Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You},
      year={2024},
      eprint={2408.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.12588},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
