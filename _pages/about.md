---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>
Hi!

I am a second-year PhD student in Computer Science at National University of Singapore advised by [Yang You](https://www.comp.nus.edu.sg/~youy/), where I also completed my master's studies. I obtained my bachelor's degree in CS & EE from Huazhong University of Science and Technology. I currently intern at [Adobe Research](https://www.adobe.com/) with [Yan Kang](https://research.adobe.com/person/yan-kang/). Previously, I collaborated at [Pika](https://pika.art/about) with [Chenlin Meng](https://cs.stanford.edu/~chenlin/) and interned at [Colossal-AI](https://github.com/hpcaitech/ColossalAI) with [Jiarui Fang](https://fangjiarui.github.io/).

My current research mainly focuses on **efficient and scalable machine learning** through algorithm and infrastructure co-optimization, recently with a primary emphasis on **efficient video generation**.


# üî• News
- *2025.07*: Join [Adobe Research](https://www.adobe.com/) as research intern in Seattle!
- *2025.05*: [DSP](https://arxiv.org/abs/2403.10266) is accepted by ICML 2025!
- *2025.01*: [PAB](https://arxiv.org/abs/2408.12588) is accepted by ICLR 2025 and integrated into [Diffusers](https://huggingface.co/docs/diffusers/en/api/cache#diffusers.PyramidAttentionBroadcastConfig)!
- *2024.03*: Release [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys) for efficient training and inference of video models!
- *2024.02*: [HeteGen](https://arxiv.org/abs/2403.01164) is accepted by MLSys 2024!
- *2024.01*: [AutoChunk](https://arxiv.org/abs/2401.10652) is accepted by ICLR 2024!
- *2024.01*: Start my PhD journey!


<span class='anchor' id='publications'></span>

# üìù Selected Publications ([all](https://scholar.google.com/citations?user=I5NBOacAAAAJ))
## üìΩÔ∏è Efficient Video Generation
<ul>
  <li>
    <code class="language-plaintext highlighter-rouge">ICLR 2025</code> <strong>Real-Time Video Generation with Pyramid Attention Broadcast</strong>
    <div style="display: inline">
        <a href="https://arxiv.org/abs/2408.12588"> [paper]</a>
        <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys"> [code]</a>
        <a href="https://oahzxl.github.io/PAB/"> [blog]</a>
    </div>
    <img src='https://img.shields.io/github/stars/NUS-HPC-AI-Lab/VideoSys.svg?style=social&label=Star' alt="sym" height="100%">
    <div><i><u>Xuanlei Zhao</u><b><sup>*</sup></b>, Xiaolong Jin<b><sup>*</sup></b>, Kai Wang<b><sup>*</sup></b>, Yang You</i></div>
  </li>
  <li>
    <code class="language-plaintext highlighter-rouge">ICML 2025</code> <strong>DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</strong>
    <div style="display: inline">
        <a href="https://arxiv.org/abs/2403.10266"> [paper]</a>
        <a href="https://github.com/NUS-HPC-AI-Lab/VideoSys"> [code]</a>
    </div>
    <div><i><u>Xuanlei Zhao</u>, Shenggan Cheng, Chang Chen, Zangwei Zheng, Ziming Liu, Zheming Yang, Yang You</i></div>
  </li>
</ul>

## üßπ Efficient Memory Cost
<ul>
  <li>
    <code class="language-plaintext highlighter-rouge">ICLR 2024</code> <strong>AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference</strong>
    <div style="display: inline">
        <a href="https://arxiv.org/abs/2401.10652"> [paper]</a>
        <a href="https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/autochunk"> [code]</a>
    </div>
    <div><i><u>Xuanlei Zhao</u>, Shenggan Cheng, Guangyang Lu, Jiarui Fang, Haotian Zhou, Bin Jia, Ziming Liu, Yang You</i></div>
  </li>
  <li>
    <code class="language-plaintext highlighter-rouge">MLSys 2024</code> <strong>HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices</strong>
    <div style="display: inline">
        <a href="https://arxiv.org/abs/2403.01164"> [paper]</a>
    </div>
    <div><i><u>Xuanlei Zhao</u><b><sup>*</sup></b>, Bin Jia<b><sup>*</sup></b>, Haotian Zhou<b><sup>*</sup></b>, Ziming Liu, Shenggan Cheng, Yang You</i></div>
  </li>
</ul>

## üî¨ Efficient AI for Science
<ul>
  <li>
    <code class="language-plaintext highlighter-rouge">PPoPP 2024</code> <strong>FastFold: Optimizing AlphaFold Training and Inference on GPU Clusters</strong>
    <div style="display: inline">
        <a href="https://dl.acm.org/doi/10.1145/3627535.3638465"> [paper]</a>
        <a href="https://github.com/hpcaitech/FastFold"> [code]</a>
    </div>
    <img src='https://img.shields.io/github/stars/hpcaitech/FastFold.svg?style=social&label=Star' alt="sym" height="100%">
    <div><i>Shenggan Cheng, <u>Xuanlei Zhao</u>, Guangyang Lu, Jiarui Fang, Tian Zheng, Ruidong Wu, Xiwen Zhang, Jian Peng, Yang You</i></div>
  </li>
</ul>

<!-- 
## Others
- `NeurIPS 2023` [Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective](https://openreview.net/forum?id=Rp4PA0ez0m), Pengfei Wei, Lingdong Kong, Xinghua Qu, **Yi Ren**, et al.
- ``ACM-MM 2022`` [Video-Guided Curriculum Learning for Spoken Video Grounding](), Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, **Yi Ren** -->


<!-- # üéñ Honors and Awards
- *2021.10* Tencent Scholarship (Top 1%)
- *2021.10* National Scholarship (Top 1%)
- *2020.12* [Baidu Scholarship](https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E5%A5%96%E5%AD%A6%E9%87%91/9929412) (10 students in the world each year)
- *2020.12* [AI Chinese new stars](https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&mid=2653639431&idx=1&sn=25b6368c1954419b9090840347d9a27d&chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&mpshare=1&scene=2&srcid=0511LMlj9Qv9DeIZAjMjYAU9&sharer_sharetime=1620731348139&sharer_shareid=631c113940cb81f34895aa25ab14422a#rd) (100 worldwide each year)
- *2020.12* [AI Chinese New Star Outstanding Scholar](https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&mid=2653639431&idx=1&sn=25b6368c1954419b9090840347d9a27d&chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&mpshare=1&scene=2&srcid=0511LMlj9Qv9DeIZAjMjYAU9&sharer_sharetime=1620731348139&sharer_shareid=631c113940cb81f34895aa25ab14422a#rd) (10 candidates worldwide each year)
- *2020.12* [ByteDance Scholars Program](https://ur.bytedance.com/scholarship) (10 students in China each year)
- *2020.10* Tianzhou Chen Scholarship (Top 1%)
- *2020.10* National Scholarship (Top 1%)
- *2015.10* National Scholarship (Undergraduate) (Top 1%) -->

# üí° Open-Source Projects

- **[VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys)**: An Easy and Efficient System for Video Generation <img src='https://img.shields.io/github/stars/NUS-HPC-AI-Lab/VideoSys.svg?style=social&label=Star' alt="sym" height="100%">

- **[Colossal-AI](https://github.com/hpcaitech/ColossalAI)**: Making large AI models cheaper, faster and more accessible <img src='https://img.shields.io/github/stars/hpcaitech/ColossalAI.svg?style=social&label=Star' alt="sym" height="100%">

- **[FastFold](https://github.com/hpcaitech/FastFold)**: Optimizing AlphaFold Training and Inference on GPU Clusters <img src='https://img.shields.io/github/stars/hpcaitech/FastFold.svg?style=social&label=Star' alt="sym" height="100%">

# üíª Internships
- *2025.07 - now*, [Adobe Research](https://www.adobe.com/), Research Intern. Mentor: [Yan Kang](https://research.adobe.com/person/yan-kang/).
- *2024 - 2024*, [Pika](https://pika.art/), Research Intern. Mentor: [Chenlin Meng](https://cs.stanford.edu/~chenlin/).
- *2022.07 - 2023.12*, [Colossal-AI](https://github.com/hpcaitech/ColossalAI), Research Intern. Mentor: [Jiarui Fang](https://fangjiarui.github.io/).


# üìñ Educations
- *2024.01 - now*, Ph.D. in Computer Science, National University of Singapore
- *2022.08 - 2023.12*, M.S. in Computer Science, National University of Singapore
- *2018.09 - 2022.06*, B.S. in Computer Science & Electrical Information, Huazhong University of Science and Technology


# üí¨ Invited Talks
- *2024.07*, Real-Time Video Generation with Pyramid Attention Broadcast, Ventures \| [\[video\]](https://www.techbeat.net/talk-info?id=892)
- *2024.06*, Speedup for Video Generation, Bytedance internal talk
